<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<title>SENTIC â€” Architecture & Technical Documentation</title>
<style>
  @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&family=JetBrains+Mono:wght@400;600&display=swap');

  :root {
    --purple: #7c3aed;
    --purple-light: #a78bfa;
    --indigo: #4f46e5;
    --emerald: #059669;
    --amber: #d97706;
    --red: #dc2626;
    --blue: #2563eb;
    --bg: #ffffff;
    --text: #111827;
    --muted: #6b7280;
    --border: #e5e7eb;
    --code-bg: #f3f4f6;
    --tag-bg: #f5f3ff;
    --tag-border: #ddd6fe;
  }

  * { box-sizing: border-box; margin: 0; padding: 0; }

  body {
    font-family: 'Inter', sans-serif;
    background: var(--bg);
    color: var(--text);
    font-size: 10.5pt;
    line-height: 1.7;
  }

  /* â”€â”€ Page layout â”€â”€ */
  .page {
    width: 210mm;
    margin: 0 auto;
    padding: 14mm 16mm 14mm 16mm;
  }

  /* â”€â”€ Cover â”€â”€ */
  .cover {
    min-height: 100vh;
    display: flex;
    flex-direction: column;
    justify-content: center;
    align-items: center;
    text-align: center;
    border-bottom: 2px solid var(--border);
    padding: 40px 0 60px;
    page-break-after: always;
  }
  .cover-badge {
    display: inline-block;
    font-size: 8pt;
    font-weight: 700;
    letter-spacing: 0.35em;
    text-transform: uppercase;
    color: var(--purple);
    background: var(--tag-bg);
    border: 1px solid var(--tag-border);
    padding: 5px 16px;
    border-radius: 999px;
    margin-bottom: 28px;
  }
  .cover h1 {
    font-size: 52pt;
    font-weight: 800;
    letter-spacing: 0.22em;
    background: linear-gradient(135deg, #7c3aed, #4f46e5);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
    margin-bottom: 10px;
    line-height: 1;
  }
  .cover-sub {
    font-size: 13pt;
    font-weight: 500;
    letter-spacing: 0.18em;
    text-transform: uppercase;
    color: var(--muted);
    margin-bottom: 36px;
  }
  .cover-tagline {
    font-size: 10.5pt;
    color: var(--muted);
    font-weight: 300;
    letter-spacing: 0.1em;
    margin-bottom: 52px;
  }
  .cover-meta {
    display: flex;
    gap: 32px;
    justify-content: center;
    flex-wrap: wrap;
  }
  .cover-meta span {
    font-size: 8.5pt;
    color: var(--muted);
    font-weight: 500;
    letter-spacing: 0.12em;
    text-transform: uppercase;
    padding: 6px 18px;
    border: 1px solid var(--border);
    border-radius: 8px;
  }

  /* â”€â”€ TOC â”€â”€ */
  .toc { page-break-after: always; padding-top: 20px; }
  .toc h2 { font-size: 15pt; font-weight: 700; color: var(--purple); margin-bottom: 22px; letter-spacing: 0.05em; }
  .toc-item {
    display: flex;
    align-items: center;
    gap: 10px;
    padding: 9px 0;
    border-bottom: 1px dotted var(--border);
    font-size: 10pt;
  }
  .toc-num {
    font-weight: 700;
    color: var(--purple);
    min-width: 28px;
    font-size: 9pt;
  }
  .toc-label { flex: 1; color: var(--text); font-weight: 500; }
  .toc-dot { flex: 1; border-bottom: 1px dotted #d1d5db; margin: 0 8px; }

  /* â”€â”€ Sections â”€â”€ */
  .section { margin-bottom: 36px; }
  .section-header {
    display: flex;
    align-items: center;
    gap: 14px;
    margin-bottom: 18px;
    padding-bottom: 10px;
    border-bottom: 2px solid var(--border);
  }
  .section-num {
    width: 34px;
    height: 34px;
    border-radius: 10px;
    background: linear-gradient(135deg, var(--purple), var(--indigo));
    color: white;
    font-weight: 800;
    font-size: 12pt;
    display: flex;
    align-items: center;
    justify-content: center;
    flex-shrink: 0;
  }
  .section-title { font-size: 14pt; font-weight: 700; color: var(--text); }
  .section-subtitle { font-size: 8.5pt; color: var(--purple); font-weight: 600; letter-spacing: 0.15em; text-transform: uppercase; }

  h3 {
    font-size: 11pt;
    font-weight: 700;
    color: var(--text);
    margin: 20px 0 10px;
    padding-left: 12px;
    border-left: 3px solid var(--purple-light);
  }
  h4 {
    font-size: 10pt;
    font-weight: 600;
    color: var(--purple);
    margin: 14px 0 7px;
    text-transform: uppercase;
    letter-spacing: 0.08em;
    font-size: 8.5pt;
  }
  p { margin-bottom: 10px; color: #374151; }

  /* â”€â”€ Architecture diagram â”€â”€ */
  .arch-diagram {
    background: #faf5ff;
    border: 1.5px solid var(--tag-border);
    border-radius: 14px;
    padding: 24px 28px;
    margin: 16px 0 20px;
    font-family: 'JetBrains Mono', monospace;
    font-size: 8pt;
    line-height: 1.9;
    color: #374151;
    white-space: pre;
  }

  /* â”€â”€ Code â”€â”€ */
  pre {
    background: var(--code-bg);
    border: 1px solid var(--border);
    border-radius: 10px;
    padding: 14px 16px;
    font-family: 'JetBrains Mono', monospace;
    font-size: 8pt;
    line-height: 1.65;
    overflow: hidden;
    margin: 12px 0 16px;
    color: #1f2937;
  }
  code {
    font-family: 'JetBrains Mono', monospace;
    font-size: 8.5pt;
    background: var(--code-bg);
    padding: 1px 5px;
    border-radius: 4px;
    color: var(--purple);
  }

  /* â”€â”€ Tables â”€â”€ */
  table {
    width: 100%;
    border-collapse: collapse;
    margin: 12px 0 18px;
    font-size: 9.5pt;
  }
  th {
    background: linear-gradient(135deg, var(--purple), var(--indigo));
    color: white;
    padding: 9px 14px;
    text-align: left;
    font-weight: 700;
    font-size: 8.5pt;
    letter-spacing: 0.05em;
  }
  th:first-child { border-radius: 8px 0 0 0; }
  th:last-child { border-radius: 0 8px 0 0; }
  td {
    padding: 8px 14px;
    border-bottom: 1px solid var(--border);
    color: #374151;
    vertical-align: top;
  }
  tr:nth-child(even) td { background: #f9fafb; }
  td:first-child { font-weight: 600; color: var(--text); }

  /* â”€â”€ Callout boxes â”€â”€ */
  .callout {
    border-radius: 12px;
    padding: 14px 18px;
    margin: 14px 0;
    display: flex;
    gap: 13px;
    align-items: flex-start;
  }
  .callout-icon { font-size: 15pt; line-height: 1; flex-shrink: 0; margin-top: 1px; }
  .callout-body { flex: 1; }
  .callout-title { font-weight: 700; font-size: 9.5pt; margin-bottom: 4px; }
  .callout-text { font-size: 9pt; line-height: 1.6; color: #374151; margin: 0; }
  .callout.info { background: #eff6ff; border: 1px solid #bfdbfe; }
  .callout.info .callout-title { color: var(--blue); }
  .callout.success { background: #f0fdf4; border: 1px solid #bbf7d0; }
  .callout.success .callout-title { color: var(--emerald); }
  .callout.warning { background: #fffbeb; border: 1px solid #fde68a; }
  .callout.warning .callout-title { color: var(--amber); }
  .callout.purple { background: #f5f3ff; border: 1px solid #ddd6fe; }
  .callout.purple .callout-title { color: var(--purple); }

  /* â”€â”€ Stage pill â”€â”€ */
  .stage-pill {
    display: inline-flex;
    align-items: center;
    gap: 8px;
    padding: 5px 14px;
    border-radius: 999px;
    font-size: 8pt;
    font-weight: 700;
    letter-spacing: 0.12em;
    text-transform: uppercase;
    margin-bottom: 12px;
  }
  .stage-pill.blue   { background: #eff6ff; color: var(--blue);    border: 1px solid #bfdbfe; }
  .stage-pill.yellow { background: #fffbeb; color: var(--amber);   border: 1px solid #fde68a; }
  .stage-pill.green  { background: #f0fdf4; color: var(--emerald); border: 1px solid #bbf7d0; }
  .stage-pill.purple { background: #f5f3ff; color: var(--purple);  border: 1px solid #ddd6fe; }

  /* â”€â”€ Flow steps â”€â”€ */
  .flow-list {
    counter-reset: flow;
    list-style: none;
    margin: 14px 0;
    padding: 0;
  }
  .flow-list li {
    counter-increment: flow;
    display: flex;
    gap: 14px;
    align-items: flex-start;
    padding: 8px 0;
    border-bottom: 1px solid #f3f4f6;
  }
  .flow-list li::before {
    content: counter(flow);
    min-width: 24px;
    height: 24px;
    border-radius: 50%;
    background: linear-gradient(135deg, var(--purple), var(--indigo));
    color: white;
    font-weight: 800;
    font-size: 8pt;
    display: flex;
    align-items: center;
    justify-content: center;
    flex-shrink: 0;
    margin-top: 1px;
  }
  .flow-list li span { font-size: 9.5pt; color: #374151; line-height: 1.6; }

  /* â”€â”€ File map â”€â”€ */
  .file-map {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 10px;
    margin: 14px 0;
  }
  .file-card {
    background: #faf5ff;
    border: 1px solid var(--tag-border);
    border-radius: 10px;
    padding: 12px 14px;
  }
  .file-card .fname {
    font-family: 'JetBrains Mono', monospace;
    font-size: 7.5pt;
    color: var(--purple);
    font-weight: 700;
    margin-bottom: 5px;
    word-break: break-all;
  }
  .file-card .frole { font-size: 8.5pt; color: #374151; line-height: 1.5; }

  /* â”€â”€ Directive box â”€â”€ */
  .directive {
    background: #ffffff;
    border: 1.5px solid var(--border);
    border-left: 4px solid var(--purple);
    border-radius: 0 10px 10px 0;
    padding: 12px 16px;
    margin: 11px 0;
  }
  .directive-title {
    font-weight: 800;
    font-size: 9pt;
    color: var(--purple);
    text-transform: uppercase;
    letter-spacing: 0.1em;
    margin-bottom: 5px;
  }
  .directive p { margin: 0; font-size: 9pt; color: #374151; }

  /* â”€â”€ Footer â”€â”€ */
  .footer {
    margin-top: 40px;
    padding-top: 14px;
    border-top: 1px solid var(--border);
    display: flex;
    justify-content: space-between;
    font-size: 7.5pt;
    color: var(--muted);
    letter-spacing: 0.08em;
  }

  page-break { page-break-before: always; }
  .pb { page-break-before: always; padding-top: 20px; }

  @media print {
    body { font-size: 10pt; }
    .page { padding: 10mm 14mm; }
    .cover { min-height: auto; padding: 30px 0 40px; }
  }
</style>
</head>
<body>
<div class="page">

  <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• COVER â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
  <div class="cover">
    <div class="cover-badge">Technical Architecture Document</div>
    <h1>SENTIC</h1>
    <div class="cover-sub">AI Text Humanization Engine</div>
    <div class="cover-tagline">Perplexity &nbsp;Â·&nbsp; Burstiness &nbsp;Â·&nbsp; Linguistic Entropy</div>
    <div class="cover-meta">
      <span>ALTRIX Platform</span>
      <span>Groq LLaMA 3.3 70B</span>
      <span>BERT Pragmatic Markers</span>
      <span>3-Stage Pipeline</span>
      <span>February 2026</span>
    </div>
  </div>

  <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• TOC â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
  <div class="toc">
    <h2>Table of Contents</h2>
    <div class="toc-item"><span class="toc-num">01</span><span class="toc-label">System Overview &amp; High-Level Architecture</span></div>
    <div class="toc-item"><span class="toc-num">02</span><span class="toc-label">Stage 0 â€” Frontend Input Layer</span></div>
    <div class="toc-item"><span class="toc-num">03</span><span class="toc-label">Stage 1 â€” Groq LLM Linguistic Entropy Pipeline</span></div>
    <div class="toc-item"><span class="toc-num">04</span><span class="toc-label">Stage 2 â€” BERT Pragmatic Marker Injection</span></div>
    <div class="toc-item"><span class="toc-num">05</span><span class="toc-label">Client-Side AI Score Estimator</span></div>
    <div class="toc-item"><span class="toc-num">06</span><span class="toc-label">Output Layer &amp; Result UI</span></div>
    <div class="toc-item"><span class="toc-num">07</span><span class="toc-label">Complete Request Flow</span></div>
    <div class="toc-item"><span class="toc-num">08</span><span class="toc-label">File Map &amp; Technology Summary</span></div>
  </div>

  <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• SECTION 1 â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
  <div class="section pb">
    <div class="section-header">
      <div class="section-num">1</div>
      <div>
        <div class="section-title">System Overview &amp; High-Level Architecture</div>
        <div class="section-subtitle">What SENTIC is and how its three stages connect</div>
      </div>
    </div>

    <p>SENTIC is the AI text humanization engine embedded in the <strong>ALTRIX platform</strong>. Its purpose is to take AI-generated text and transform it â€” statistically and stylistically â€” into output that is indistinguishable from genuine human writing, thereby evading modern AI-detection classifiers such as GPTZero, Turnitin, and Winston AI.</p>
    <p>It accomplishes this through a <strong>3-stage pipeline</strong> that attacks AI detectability from three completely distinct angles simultaneously:</p>

    <div class="arch-diagram">USER INPUT (text / file / voice)
         â”‚
         â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚        SENTIC Frontend  (Next.js)              â”‚  src/.../sentic/page.tsx
 â”‚   Â·  Audience selector                         â”‚
 â”‚   Â·  Entropy Level selector (Lowâ†’Max)          â”‚
 â”‚   Â·  Text Settings modal (6 toggles)           â”‚
 â”‚   Â·  AI Score Estimator (client-side)          â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚  POST /api/humanize
                     â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   STAGE 1 â€” Next.js API Route                  â”‚  src/app/api/humanize/route.ts
 â”‚   Groq LLM  Â·  LLaMA 3.3 70B Versatile       â”‚
 â”‚   Linguistic Entropy Rewrite                   â”‚
 â”‚   Â· AI-ism pre-strip                           â”‚
 â”‚   Â· Sentence chunker (300-word chunks)         â”‚
 â”‚   Â· Master prompt: Burstiness + Syntax         â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚  POST http://127.0.0.1:8000/inject-markers
                     â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   STAGE 2 â€” Python FastAPI Backend             â”‚  ai-humanizer/fastapi_app.py
 â”‚   BERT Pragmatic Marker Injector               â”‚  ai-humanizer/humanizer.py
 â”‚   Â· 28% probabilistic injection                â”‚
 â”‚   Â· Capped at 25% of sentences                 â”‚
 â”‚   Â· 3-second timeout (graceful degrade)        â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚      Humanized Text Output   â”‚
      â”‚   + AI Score Badge           â”‚
      â”‚   + Pipeline Log             â”‚
      â”‚   + Entropy Parameters       â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</div>

    <div class="callout purple">
      <div class="callout-icon">ğŸ§©</div>
      <div class="callout-body">
        <div class="callout-title">Why Three Stages?</div>
        <p class="callout-text">Stage 1 handles macro-level restructuring (burstiness, vocabulary, syntax). Stage 2 handles micro-level pragmatic fingerprinting (discourse markers). The client-side score estimator provides immediate feedback without needing another API call. Each layer is independently effective and additively powerful.</p>
      </div>
    </div>
  </div>

  <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• SECTION 2 â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
  <div class="section pb">
    <div class="section-header">
      <div class="section-num">2</div>
      <div>
        <div class="section-title">Stage 0 â€” Frontend Input Layer</div>
        <div class="section-subtitle">page.tsx Â· Next.js React Client Component</div>
      </div>
    </div>
    <div class="stage-pill blue">ğŸ”µ Stage 0 Â· Frontend Â· page.tsx</div>

    <h3>Input Methods</h3>
    <p>SENTIC supports four distinct ways to get text into the system:</p>
    <table>
      <tr><th>Method</th><th>Implementation</th><th>Supported Formats</th></tr>
      <tr><td>Manual Typing</td><td>Plain controlled <code>&lt;textarea&gt;</code></td><td>Any text</td></tr>
      <tr><td>Paste</td><td><code>navigator.clipboard.readText()</code></td><td>Any clipboard text</td></tr>
      <tr><td>File Upload</td><td><code>parsePdf / parseDocx / parseTxt / fileToBase64</code></td><td>.txt, .pdf, .docx, images</td></tr>
      <tr><td>Voice Input</td><td><code>webkitSpeechRecognition</code> (continuous, interim results)</td><td>Live microphone audio</td></tr>
    </table>

    <h3>Core User Controls</h3>
    <h4>1 â€” Audience Selector</h4>
    <p>Options: <code>Student</code> | <code>Professional</code> | <code>General Reader</code> | <code>Expert</code>. Sent to the API as the <code>audience</code> parameter â€” informs the LLM of the target readership so that register and tone are preserved appropriately.</p>

    <h4>2 â€” Entropy Level (The Primary Humanization Knob)</h4>
    <p>Maps directly to Groq sampling parameters. Higher entropy produces more unpredictable, varied output, making AI detection harder:</p>
    <table>
      <tr><th>Level</th><th>Temperature</th><th>top_p</th><th>Use Case</th></tr>
      <tr><td>Low</td><td>0.75</td><td>0.85</td><td>Subtle restructuring Â· Safe for academic tone</td></tr>
      <tr><td>Medium</td><td>0.88</td><td>0.90</td><td>Balanced burstiness Â· Good general use</td></tr>
      <tr><td>High</td><td>0.95</td><td>0.93</td><td>Aggressive variance Â· Strong anti-detection</td></tr>
      <tr><td>Max</td><td>1.02</td><td>0.97</td><td>Maximum unpredictability Â· Use with care</td></tr>
    </table>

    <h4>3 â€” Text Post-Processing Settings (6 Toggles)</h4>
    <p>Applied <em>after</em> LLM output as cleanup, not during generation. All regex-based, purely cosmetic:</p>
    <table>
      <tr><th>Toggle</th><th>Regex / Operation</th><th>Purpose</th></tr>
      <tr><td>Remove Unicode</td><td><code>/[^\x00-\x7F]/g â†’ " "</code></td><td>Strip hidden non-ASCII chars</td></tr>
      <tr><td>Dashes â†’ Commas</td><td><code>\s*[-â€“]\s* â†’ ", "</code></td><td>Convert list dashes to commas</td></tr>
      <tr><td>Remove Dashes</td><td><code>/[-â€“â€”]/g â†’ " "</code></td><td>Strip all dash characters</td></tr>
      <tr><td>Transform Quotes</td><td><code>/[""'']/g â†’ '"'</code></td><td>Normalize curly quotes</td></tr>
      <tr><td>Remove Whitespace</td><td><code>/\s{2,}/g â†’ " "</code></td><td>Collapse double spaces</td></tr>
      <tr><td>Remove Em-dash</td><td><code>/â€”/g â†’ " "</code></td><td>Strip em-dashes</td></tr>
    </table>
  </div>

  <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• SECTION 3 â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
  <div class="section pb">
    <div class="section-header">
      <div class="section-num">3</div>
      <div>
        <div class="section-title">Stage 1 â€” Groq LLM Linguistic Entropy Pipeline</div>
        <div class="section-subtitle">route.ts Â· Next.js API Route Â· LLaMA 3.3 70B Versatile</div>
      </div>
    </div>
    <div class="stage-pill yellow">ğŸŸ¡ Stage 1 Â· Groq LLM Â· route.ts</div>

    <h3>Pre-Processing: Banned AI-isms Strip</h3>
    <p>Before the text ever reaches the LLM, a list of high-frequency AI signature words is stripped via regex word-boundary matching. This prevents the model from seeing and regenerating these terms:</p>
<pre>const AI_ISMS = [
  "comprehensive", "foster", "delve", "tap into", "multifaceted",
  "underscores", "underscore", "testament", "realm", "pivotal",
  "intricate", "democratize", "game-changer", "unleash", "leverage",
  "holistic", "it is worth noting", "furthermore", "moreover",
  "in addition", "additionally", "it is important to note"
]
// Matched with: /\bword\b/gi â€” word boundary aware</pre>

    <h3>Chunking for Long Texts</h3>
    <p>If the cleaned input exceeds 300 words, it is split at sentence boundaries into ~300-word chunks. Each chunk is processed independently and results are joined. This prevents context window overflow and maintains quality consistency.</p>
<pre>function chunkText(text: string, targetWords = 300): string[]
// Splits by sentence regex: /[^.!?]+[.!?]+[\s]*/g
// Each chunk sent independently to Groq, results joined with " "</pre>

    <h3>The Master Prompt â€” SYSTEM_PROMPT</h3>
    <p>This is the most critical engineering element in SENTIC. Five hard directives are given to the LLM:</p>

    <div class="directive">
      <div class="directive-title">Directive 1 â€” Burstiness (Most Critical)</div>
      <p>Vary sentence lengths aggressively: SHORT (3â€“6 words) â†’ LONG (30â€“50 words) â†’ MEDIUM (10â€“18 words). Never write 3 sentences of similar length in a row. <strong>Why:</strong> AI detectors measure the statistical variance (burstiness) of sentence lengths. AI output is uniform; human writing is not.</p>
    </div>

    <div class="directive">
      <div class="directive-title">Directive 2 â€” Lateral Vocabulary</div>
      <p>No thesaurus swaps. Replace single words with phrases: <em>"significant"</em> â†’ <em>"it carries weight"</em>. Hard ban list applied. <strong>Why:</strong> Simple synonym replacement creates awkward "Frankenstein" text that raises, not lowers, detection scores.</p>
    </div>

    <div class="directive">
      <div class="directive-title">Directive 3 â€” Human Micro-Transitions</div>
      <p>NEVER use: Furthermore, Moreover, In addition, Additionally, It is worth noting.<br>USE instead: "But here's the thing â€”", "That aside,", "The catch is,", "Oddly enough,", "In practice though,". <strong>Why:</strong> These connectors are AI signature phrases detectable at extremely high signal strength.</p>
    </div>

    <div class="directive">
      <div class="directive-title">Directive 4 â€” Syntactic Front-Loading</div>
      <p>Never start sentences with "The [Noun]...". Begin with prepositional phrases, dependent clauses, adverbs ("Frankly,"), or gerunds. <strong>Why:</strong> LLMs default obsessively to Subject + Verb structure. Inverted syntax raises perplexity naturally.</p>
    </div>

    <div class="directive">
      <div class="directive-title">Directive 5 â€” Preserve Technical Terms &amp; Rich Punctuation</div>
      <p>Only humanise connective tissue, not domain content. Use em-dashes (â€”), parentheses, and semicolons throughout â€” these are statistically rare in AI output and very common in human writing.</p>
    </div>

    <h3>Groq API Call Parameters</h3>
    <table>
      <tr><th>Parameter</th><th>Value</th><th>Purpose</th></tr>
      <tr><td>model</td><td><code>llama-3.3-70b-versatile</code></td><td>Groq's fastest high-quality model</td></tr>
      <tr><td>temperature</td><td>0.75 â€“ 1.02</td><td>Sampling randomness (set by Entropy Level)</td></tr>
      <tr><td>top_p</td><td>0.85 â€“ 0.97</td><td>Nucleus sampling cutoff (set by Entropy Level)</td></tr>
      <tr><td>frequency_penalty</td><td>0.35</td><td>Reduces word repetition</td></tr>
      <tr><td>max_tokens</td><td>1536</td><td>Output length cap per chunk</td></tr>
    </table>
  </div>

  <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• SECTION 4 â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
  <div class="section pb">
    <div class="section-header">
      <div class="section-num">4</div>
      <div>
        <div class="section-title">Stage 2 â€” BERT Pragmatic Marker Injection</div>
        <div class="section-subtitle">humanizer.py + fastapi_app.py Â· Python FastAPI on port 8000</div>
      </div>
    </div>
    <div class="stage-pill green">ğŸŸ¢ Stage 2 Â· BERT FastAPI Â· port 8000</div>

    <div class="callout warning">
      <div class="callout-icon">â±</div>
      <div class="callout-body">
        <div class="callout-title">Graceful Degradation</div>
        <p class="callout-text">Stage 2 runs with a hard 3-second timeout. If the Python server is offline or slow, this stage is silently skipped and Stage 1 output is returned directly. The pipeline is fully functional without the Python backend.</p>
      </div>
    </div>

    <h3>Why NOT the Old BERT MLM Approach</h3>
    <p>The original SENTIC used BERT's Masked Language Model (MLM) to swap tokens in the text with synonym predictions. This was <strong>counterproductive</strong> for two reasons:</p>
    <ul style="margin: 8px 0 14px; padding-left: 20px; color: #374151; font-size: 9.5pt; line-height: 1.8;">
      <li>BERT MLM fills masked positions with the <em>most probable</em> token â€” which is precisely what AI detectors look for (low perplexity).</li>
      <li>Token-level synonym replacement creates semantically awkward "Frankenstein" text that actually <strong>raises</strong> AI detection scores.</li>
    </ul>

    <h3>What BERT Does Now (Encoder Only)</h3>
    <p>BERT's encoder (<code>bert-base-uncased</code>) is loaded in <strong>evaluation mode with no MLM head</strong>. It is effectively not used for inference â€” its presence is a legacy shim. The real work is done by pure Python logic: probabilistic pragmatic marker injection.</p>

    <h3>Pragmatic Marker Pools</h3>
<pre>MARKERS_HEDGING     = ["Admittedly,", "To be fair,", "That said,", "In fairness,"]
MARKERS_EMPHASIS    = ["Frankly,", "Honestly,", "Put simply,", "In plain terms,"]
MARKERS_TRANSITION  = ["In practice,", "The catch is,", "What this means â€”"]
MARKERS_CONTRAST    = ["Oddly enough,", "Against expectations,", "Counterintuitively,"]
MARKERS_CONSEQUENCE = ["As a result,", "The consequence is clear:", "So it follows that"]</pre>

    <h3>Injection Algorithm</h3>
<pre>def inject_pragmatic_markers(text: str, injection_rate: float = 0.28) -> str:</pre>
    <p>For every sentence in the text, the algorithm:</p>
    <ol style="margin: 8px 0 14px; padding-left: 20px; color: #374151; font-size: 9.5pt; line-height: 2;">
      <li>Checks <code>_needs_marker()</code>: sentence must be &gt; 6 words AND first 60 chars must not already contain a discourse signal from the <code>EXISTING_SIGNALS</code> list (prevents double-injection).</li>
      <li>Rolls the dice: <code>random.random() &lt; 0.28</code> â€” 28% probability of injection per eligible sentence.</li>
      <li>Caps total injections at 25% of all sentences (<code>cap = len(sentences) // 4</code>) to prevent over-engineering.</li>
      <li>Prepends a randomly chosen marker from the pool and lowercases the first letter of the original sentence to maintain flow.</li>
    </ol>

    <div class="callout success">
      <div class="callout-icon">âœ…</div>
      <div class="callout-body">
        <div class="callout-title">Result</div>
        <p class="callout-text">~25% of eligible sentences receive natural-feeling human discourse markers. These markers carry near-zero semantic weight but high stylistic fingerprint, making text appear more human without distorting meaning.</p>
      </div>
    </div>

    <h3>FastAPI Endpoints</h3>
    <table>
      <tr><th>Endpoint</th><th>Method</th><th>Purpose</th></tr>
      <tr><td><code>GET /</code></td><td>GET</td><td>Health check</td></tr>
      <tr><td><code>POST /inject-markers</code></td><td>POST</td><td>Stage 2: pragmatic marker injection (primary endpoint)</td></tr>
      <tr><td><code>POST /humanize</code></td><td>POST</td><td>Legacy alias â†’ routes to inject-markers</td></tr>
      <tr><td><code>POST /extract-text</code></td><td>POST</td><td>PDF/DOCX text extraction for document upload</td></tr>
    </table>
  </div>

  <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• SECTION 5 â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
  <div class="section pb">
    <div class="section-header">
      <div class="section-num">5</div>
      <div>
        <div class="section-title">Client-Side AI Score Estimator</div>
        <div class="section-subtitle">estimateAiScore() Â· Pure TypeScript Â· Zero API calls Â· Real-time</div>
      </div>
    </div>
    <div class="stage-pill purple">ğŸŸ£ Client-Side Â· Zero-API Â· Real-Time</div>

    <p>A fully local, zero-API measurement function that runs in the browser and updates live as the humanized text changes. It provides immediate, actionable feedback without any network request.</p>

    <h3>Scoring Formula</h3>
    <p>Score starts at 100 (perfectly AI-like). Operations reduce it. Lower = more human:</p>
    <table>
      <tr><th>Signal</th><th>Calculation</th><th>Direction</th></tr>
      <tr><td>Sentence Length Burstiness (CV)</td><td><code>score -= Math.min(cv * 60, 50)</code></td><td>â†“ More variance = lower score (good)</td></tr>
      <tr><td>AI-ism Hits (banned words)</td><td><code>score += hits * 12</code></td><td>â†‘ Each banned word raises score (bad)</td></tr>
      <tr><td>Punctuation Richness (â€”, (, ;)</td><td><code>score -= richness * 6</code> (cap: 6)</td><td>â†“ Rich punctuation = lower score (good)</td></tr>
    </table>

    <p><strong>Burstiness CV</strong> is the coefficient of variation of sentence word-lengths: standard deviation divided by mean. A high CV means sentence lengths vary widely â€” which is the primary human writing signal.</p>

    <h3>Score Bands</h3>
    <table>
      <tr><th>Score Range</th><th>Level</th><th>Label</th></tr>
      <tr><td>0 â€“ 34</td><td>ğŸŸ¢ Low</td><td>Low AI Signal â€” likely to pass detection</td></tr>
      <tr><td>35 â€“ 64</td><td>ğŸŸ¡ Medium</td><td>Medium AI Signal â€” borderline</td></tr>
      <tr><td>65 â€“ 100</td><td>ğŸ”´ High</td><td>High AI Signal â€” likely to be flagged</td></tr>
    </table>

    <p>The detail line example: <code>Burstiness CV: 0.42 Â· âœ“ No banned words Â· Punctuation richness: 3</code></p>
  </div>

  <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• SECTION 6 â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
  <div class="section">
    <div class="section-header">
      <div class="section-num">6</div>
      <div>
        <div class="section-title">Output Layer &amp; Result UI</div>
        <div class="section-subtitle">Five output panels rendered after pipeline completion</div>
      </div>
    </div>

    <table>
      <tr><th>#</th><th>Panel</th><th>Contents</th></tr>
      <tr><td>1</td><td>Humanized Version</td><td>Read-only textarea Â· word/char count pills Â· Listen (TTS) Â· Copy Â· Export (PDF/DOCX/TXT) Â· Retry</td></tr>
      <tr><td>2</td><td>AI Score Estimator Badge</td><td>Live score bar + level label + detail breakdown (CV, AI-isms, punctuation)</td></tr>
      <tr><td>3</td><td>Pipeline Log</td><td>Expandable Â· Shows stage-by-stage status: âœ“ or skipped reason</td></tr>
      <tr><td>4</td><td>Entropy Parameters</td><td>Expandable Â· Echoes: Voice, Audience, Entropy Level, Intent</td></tr>
      <tr><td>5</td><td>Explainability Log</td><td>Expandable Â· Techniques, Voice, Structural Changes, Semantic Integrity</td></tr>
    </table>

    <h3>Pipeline Log Examples</h3>
<pre>// Both stages successful:
Stage 1 (Entropy Rewrite): âœ“ (3 chunks) | Stage 2 (BERT Markers): âœ“

// Python backend offline:
Stage 1 (Entropy Rewrite): âœ“ | Stage 2 (BERT): skipped (offline/timeout)

// Partial chunk failure:
Stage 1: partial âœ— (some chunks fell back) | Stage 2 (BERT Markers): âœ“</pre>
  </div>

  <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• SECTION 7 â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
  <div class="section pb">
    <div class="section-header">
      <div class="section-num">7</div>
      <div>
        <div class="section-title">Complete Request Flow</div>
        <div class="section-subtitle">End-to-end walkthrough of a single humanization request</div>
      </div>
    </div>

    <ol class="flow-list">
      <li><span>User types, pastes, uploads, or voices text into the input panel.</span></li>
      <li><span>User selects <strong>Audience</strong> (Student / Professional / General Reader / Expert).</span></li>
      <li><span>User selects <strong>Entropy Level</strong> (Low / Medium / High / Max).</span></li>
      <li><span>User clicks <strong>"Humanize Text"</strong> button.</span></li>
      <li><span>Frontend sends <code>POST /api/humanize</code> with <code>{ text, audience, formality, mode: "humanize" }</code>.</span></li>
      <li><span><code>route.ts</code> strips all AI-isms from the input text using word-boundary regex.</span></li>
      <li><span>Entropy level maps to <code>temperature</code> and <code>top_p</code> values via <code>entropyParams()</code>.</span></li>
      <li><span>If text &gt; 300 words, <code>chunkText()</code> splits it into sentence-boundary-aware chunks.</span></li>
      <li><span>Each chunk is sent to Groq's LLaMA 3.3 70B with the SYSTEM_PROMPT + user directive. Chunks run sequentially; failed chunks fall back to original text.</span></li>
      <li><span>All chunk outputs are joined into <code>stage1Text</code>.</span></li>
      <li><span>Next.js route calls <code>POST http://127.0.0.1:8000/inject-markers</code> with a 3-second AbortController timeout.</span></li>
      <li><span>Python server walks every sentence: eligible sentences receive a random pragmatic marker with 28% probability, capped at 25% of all sentences.</span></li>
      <li><span><code>finalText</code> (with or without markers) returned to Next.js route.</span></li>
      <li><span>Route packages and returns <code>{ humanizedText, analysis, explainability }</code> JSON.</span></li>
      <li><span>Frontend applies any active Text Settings toggles (regex post-processing).</span></li>
      <li><span>All 5 output panels render: Humanized Text, AI Score Badge, Pipeline Log, Entropy Parameters, Explainability Log.</span></li>
    </ol>
  </div>

  <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• SECTION 8 â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
  <div class="section">
    <div class="section-header">
      <div class="section-num">8</div>
      <div>
        <div class="section-title">File Map &amp; Technology Summary</div>
        <div class="section-subtitle">Source files, dependencies, and tech stack</div>
      </div>
    </div>

    <h3>Source Files</h3>
    <div class="file-map">
      <div class="file-card">
        <div class="fname">src/app/(dashboard)/sentic/page.tsx</div>
        <div class="frole">Frontend UI â€” all controls, React state, input methods (typing, paste, upload, voice), output panels, TTS, export</div>
      </div>
      <div class="file-card">
        <div class="fname">src/app/api/humanize/route.ts</div>
        <div class="frole">Stage 1 â€” AI-ism stripping, chunking, Groq API orchestration, BERT call, response packaging</div>
      </div>
      <div class="file-card">
        <div class="fname">ai-humanizer/fastapi_app.py</div>
        <div class="frole">Stage 2 Python server â€” FastAPI app, CORS, /inject-markers endpoint, /extract-text endpoint</div>
      </div>
      <div class="file-card">
        <div class="fname">ai-humanizer/humanizer.py</div>
        <div class="frole">Stage 2 core logic â€” BERT encoder loader, marker pools, _needs_marker(), inject_pragmatic_markers()</div>
      </div>
      <div class="file-card">
        <div class="fname">ai-humanizer/document_reader.py</div>
        <div class="frole">PDF/DOCX text extraction â€” used by /extract-text endpoint for document upload support</div>
      </div>
      <div class="file-card">
        <div class="fname">src/components/ExportDropdown.tsx</div>
        <div class="frole">Export panel â€” outputs humanized text as PDF, DOCX, or plain TXT file download</div>
      </div>
    </div>

    <h3>Technology Stack</h3>
    <table>
      <tr><th>Layer</th><th>Technology</th><th>Purpose</th></tr>
      <tr><td>Frontend Framework</td><td>Next.js 14 (App Router)</td><td>React server/client components, API routes</td></tr>
      <tr><td>UI Animation</td><td>Framer Motion</td><td>All transitions, AnimatePresence, motion.div</td></tr>
      <tr><td>LLM Provider</td><td>Groq Cloud API</td><td>Ultra-fast LLaMA 3.3 70B inference</td></tr>
      <tr><td>LLM Model</td><td>llama-3.3-70b-versatile</td><td>Stage 1 linguistic entropy rewrite</td></tr>
      <tr><td>Backend Framework</td><td>Python FastAPI + Uvicorn</td><td>Stage 2 marker injection server</td></tr>
      <tr><td>NLP Model</td><td>BERT (bert-base-uncased)</td><td>Encoder only â€” legacy; real work is rule-based</td></tr>
      <tr><td>Voice Input</td><td>Web Speech API (webkit)</td><td>Continuous speech recognition</td></tr>
      <tr><td>TTS Output</td><td>Web Speech API</td><td>SpeechSynthesisUtterance for listen mode</td></tr>
      <tr><td>Icons</td><td>Lucide React</td><td>All UI icons</td></tr>
    </table>

    <div class="callout info">
      <div class="callout-icon">ğŸ’¡</div>
      <div class="callout-body">
        <div class="callout-title">Anti-Detection Strategy Summary</div>
        <p class="callout-text">SENTIC attacks AI detection from three independent axes: <strong>(1)</strong> Macro-structural reshaping via LLM (burstiness, vocabulary, syntax) â€” targets statistical classifiers. <strong>(2)</strong> Micro-pragmatic fingerprinting via BERT stage (discourse markers) â€” targets stylometric classifiers. <strong>(3)</strong> Pre-processing AI-ism elimination â€” removes the highest-signal vocabulary features before any model even sees the text.</p>
      </div>
    </div>
  </div>

  <!-- â”€â”€ Footer â”€â”€ -->
  <div class="footer">
    <span>SENTIC Â· ALTRIX Platform Â· Technical Architecture Document</span>
    <span>Generated February 2026</span>
  </div>

</div>

<script>
  // Auto print on load if ?print=1 in URL
  if (window.location.search.includes('print=1')) {
    window.onload = () => window.print();
  }
</script>
</body>
</html>
